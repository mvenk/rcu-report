\documentclass[a4paper]{article}
\usepackage[pdftex]{graphicx}
\usepackage{amssymb}
\usepackage{fancyvrb}
\usepackage[procnames]{listings}
\usepackage{epstopdf}
\usepackage{courier}
\usepackage{hyperref}
\usepackage{mathptmx}
\usepackage{examplep}
\usepackage{cprotect}
\usepackage{calc}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{color}
%margins
% Margin width
\newlength{\marginwidth}
\setlength{\marginwidth}{\marginparwidth}
\addtolength{\marginwidth}{\marginparsep}
\definecolor{gray}{gray}{0.5}
\definecolor{darkgray}{gray}{0.3}
\lstset {
language=C++,
basicstyle=\footnotesize\ttfamily,
showstringspaces=false,
numbers=left,
frame=none,
numbersep=5pt,
captionpos=b,
xleftmargin=17pt,
framexleftmargin=17pt,
framexrightmargin=5pt,
framexbottommargin=4pt,
stringstyle=\color{gray}\ttfamily,
commentstyle=\color{gray}\slshape,
keywordstyle=\color{black}\bfseries,
procnamekeys={RadixIPLookup,Master,ReclaimHook},
indexprocnames=true,
procnamestyle=\color{darkgray}\bfseries,
numberstyle=\tiny
}

\hypersetup{
 colorlinks=true,
 linkcolor=black,
 citecolor=black,
 urlcolor=blue,
}
\newcommand{\INDSTATE}[1][1]{\STATE\hspace{#1\algorithmicindent}}
\DefineVerbatimEnvironment{code}{Verbatim}{fontsize=\normalsize}{\fontfamily{pcr}}
\frenchspacing
\hyphenation{free-list Ra-dix-IP-Look-up Pound-Rad-ix-IP-Look-up Bash-Rad-ix-IP-Look-up Read-Rad-ix-IP-Look-up Router-thread}
\begin{document}

\include{title}
\include{abstract}
\include{acknowledgements}

\tableofcontents

\pagebreak
\section{Introduction}
The ever-increasing volume of network traffic over the internet demands highly efficient routing lookups. Apart from performing route lookups, routers are also required to support a variety of other functions such as encryption, classification, inspection and filtering \cite{routebricks}. When compared to hardware routers, software routers are more flexible and easily extensible for integrating and adding such functionality. However software routers struggle to comfortably scale beyond 10 Gbps \cite{routebricks}, being far outpaced by line rates which exceed 40 Gbps \cite{attlinkrate}.\\

Route lookups are a significant challenge for software router performance and scalability. A route lookup minimally involves searching a large routing table for the correct destination port per packet. Routing table designs have remained an active area of research \cite{fastrtable1,fastrtable2,fastrtable3} as routing tables continue to increase in size \cite{incrtablesize}. A routing table can have over 366,000 routes \cite{routeviews}. Routing tables are read-write data structures with reads far outnumbering writes. Every forwarded packet involves reading the routing table; updates are far fewer in comparison. \\

Commodity machines with SMP multi-core architectures are growing in popularity. A software modular router such as Click \cite{click} running on commodity machines  can benefit greatly by exploiting parallelism on multi-core architectures. Building a scalable and safe multi-core application is not a trivial task. Shared data structures in a multi-threaded application require synchronization for safe and correct access. Synchronization introduces overheads which attenuate the benefits of parallelism. Locks ultimately have the effect of  serialization such that only one thread accesses shared data at a time. As our goal is to support a fast lookup performance, a conventional locking solution which acquires a lock for every route lookup would be inefficient. Reader-writer locks which allow readers to proceed concurrently with other readers have the undesirable property of excluding readers in the presence of an active concurrent updater.\\

We wish to design a highly efficient multi-core solution for IP lookups in a software router. This workload is typically read intensive---comprising 90\% readers and less than 10\% updaters. \emph{Read-Copy Update} (RCU) \cite{readcopyupdate} scales efficiently on read intensive workloads. This is because RCU has very low read side overhead, allowing concurrent readers to execute in the presence of a writer without any synchronization. Concurrent writers generally require some form of synchronization. ``RCU permits both readers and writers to make forward concurrent progress'' \cite{urcu}. For these reasons, we implemented an RCU framework for the Click modular router. We compare the performance of our approach against a traditional reader-writer lock for a read intensive workload with few updates ($\le$ 5\% of the total workload) and show that RCU performs up to 27 times faster.\\

The rest of the document is organized as follows. Section \ref{sec:background} gives the reader a background on the userlevel Click elements we are concerned with. Section \ref{sec:problem} describes why implementing a multicore solution was challenging. Section \ref{sec:solution} describes our solutions to the problem along with the implementation details. Section \ref{sec:perfeval} describes the performance analysis.
\section{Related Work}
With over 2000 reported uses of the RCU API \cite{rcuusage} in the Linux kernel in recent years, RCU is widely used in system software. It does not seem to be as popular in userlevel applications. Desnoyers et al. \cite{urcu} attribute this to multiple factors, such as the constrained design imposed by previous userlevel RCU algorithms, heavy read-side overhead, etc. They propose and discuss several approaches to implementing RCU in applications. One such approach is the quiescent state based reclamation (QSBR) approach. A thread reaches a quiescent state when it releases references to mutable data structures. When each thread has reached a quiescent state, QSBR reclaims stale references. It has near zero read side overhead but has constraints on application design: when the thread lies in a quiescent state, it is required to call \verb+rcu_quiescent_state()+. Another approach is what is referred to as \emph{General Purpose RCU}. In contrast with QSBR, this approach does not impose any constraints on application design; it can even be used in implementing library functions and API. It has the highest read-side overhead as compared to the other RCU approaches. The overhead is due to memory barriers which are inserted before and after the read side critical sections. The third approach is the \emph{Signal-Based RCU}. It avoids the high read-side overhead in the general-purpose approach by sending POSIX signal from the update side critical sections.

Click has natural quiescent states which allow us to implement QSBR. Each thread periodically calls a function which checks for a grace period in order to begin reclaiming stale data. Section \ref{sec:rcuschedloop} describes this mechanism in detail.\\

Hart et al. \cite{hart} compare several different memory reclamation schemes for RCU. Amongst these QSBR is again the scheme with the best performance, but is constrained by the requirement that the application has some natural quiescent states. The authors mention that the QSBR approach can be implemented through a fuzzy barrier. ``A barrier protects access to some code which no thread should execute before all other threads finish some prior stage of computation.'' A fuzzy barrier is non-blocking, in contrast with a (non-fuzzy) barrier. If a thread cannot get past a fuzzy barrier, it skips the protected code and resumes execution. A (non-fuzzy) barrier would have the thread block until all threads have reached it. We describe how we implement the fuzzy barrier in Section \ref{sec:rcu}. The epoch based reclamation (EBR) scheme is application independent, but has higher overhead than QSBR. EBR is similar to the general purpose RCU approach described by Hart et al. \cite{urcu}, in the sense that it is application agnostic. EBR has some read side overhead where QSBR has none. EBR uses epochs to implement a fuzzy barrier. Each thread updates local epoch numbers which are checked against a global epoch number to detect quiescent states. Although our approach is modeled on QSBR, it is similar to EBR for several reasons. Firstly, we use epochs to implement a fuzzy barrier. Secondly updaters and \emph{readers} update a local epoch number as opposed to a pure QSBR approach where readers have zero overhead.\\

Dobrescu et al. \cite{routebricks} in \emph{RouteBricks} propose an architecture which enables higher packet processing speeds by distributing router functionality across multiple servers and multiple cores within a server. Their approach involves load balancing through Valiant Load Balancing (VLB). Their method of achieving faster packet processing is at a coarser level of granularity when compared to our approach. Our approach complements RouteBricks, since a RouteBricks router could potentially achieve faster packet forwarding rates by using our RCU algorithms on its component bricks.\\

Chen and Morris \cite{chenmorris} describe techniques for multiprocessor routing by optimizing scheduling, buffer and device management. Their technique uses a per CPU routing table which does not allow dynamic updates and deletions. Synchronization for other elements is achieved through reader-writer locks or through the atomic \texttt{xchgw} instruction. Our work can complement theirs as it provides a synchronization framework for all Click elements.\\

Multiple efforts have been made at exploiting parallelism in the packet-flow mechanism in a software router. Wu and Wolf \cite{runtimepacketprocessing} have designed a run-time system which distributes the allocation of processing tasks to processor cores. It also balances the workload and maps the it across multiple cores. This parallelism is done at a task-level granularity, and does not deal with synchronization of shared data structures used within a task. In that respect, our work complements that of Wu and Wolf \cite{runtimepacketprocessing}, since it ensures that tasks are thread-safe.\\ 
%It does not look at synchronization of objects being used by the task. In our work we attempt to allow the task itself to be thread-safe so that it can be run on multiple cores.\\


Dobrescu et al. \cite{dobrescu} have proposed a technique in which the packet-processing flow and server characteristics are taken as input to formulate an optimization problem. A compiler would then solve this problem and output machine code which will optimize for throughput.
\section{Background}
\label{sec:background}
This section presents a brief overview of the \emph{RadixIPLookup} Element. Our approach involved implementation and analysis using the RadixIPLookup Element. We then extended our solution to other elements in Click. 
\subsection{An Overview of RadixIPLookup}
The \emph{RadixIPLookup} Element \cite{radixiplookup} is a class which is used to perform IPv4 route lookups. The routing table involves a radix trie to perform route lookups and a vector to store the route information such as the port and gateway. This structure is explained in further detail. 
\subsubsection{The Radix Tree}
The radix tree is structured as a trie. The trie has  $2^8$ nodes at the first level, and $2^4$ nodes in the subsequent 6 levels. A lookup therefore has to traverse a maximum of 7 levels. This structure covers all $2^{32}$ IP addresses ($2^8\times(2^4)^6=2^{32}$) when the last level is fully occupied. Figure \ref{radixtree} shows the tree pictorially.\\
\begin{figure}[tph]
\begin{center}
\includegraphics[scale = 0.23]{../images/diagrams/radixtree2.eps}
\caption{The Radix Tree}
\label{radixtree}
\end{center}
\end{figure}
\\Each level consists of radix nodes. The structure of the radix node is shown in Listing \ref{radixnode}. \verb+_n+ represents the node's fanout (the number of keys it contains). \verb+_bitshift+ represents the number of bits to be shifted at that level for prefix matching. For the radix node at level 0, \verb+_n+ is 256 and \verb+_bitshift+ is 24 (which is $32 - 8$). This means that the first 8 bits of the address are matched. For the radix nodes at level $1$, \verb+_n+ is 16 and \verb+_bitshift+ is 20 (which is $32 -8 -4$). This means that at level 1, the first 12 bits of the address have been matched.
\begin{lstlisting}[float=tph, caption = The Radix Class, label=radixnode]
class Radix {
  private:
    int _bitshift;
    int _n;
    int _nchildren;
    struct Child {
        int key;
        Radix *child;
    } _children[0];
...
...
}
\end{lstlisting}
The functions \verb+add_route()+ and \verb+delete_route()+ are used to add and delete routes into the routing table.
\paragraph{Lookups}
RadixIPLookup uses a longest prefix match algorithm \cite{rfc2461}, matching a candidate route with the highest subnet mask. The node at the topmost level stores keys which need to match up to an 8 bit long prefix. The nodes in the next level store keys for routes which need to match up to a $12$ bit long prefix, and so on. The last level stores keys which can match up to a $32$ bit long prefix. Listing \ref{lookup} shows the code for the lookup. Given an IP address \emph{addr} which we need to lookup, we calculate the child pointer of the radix node which we need to follow using the \emph{bitshift} at that level. If the key for that child exists, we repeat the process, trying to obtain the longest prefix match. If it does not exist, we have found the longest prefix match for that node, so we return the key value. This key indexes into a vector which will return the gateway and port for the address.
\begin{lstlisting}[caption = The lookup function, label=lookup]
  static inline int lookup(const Radix *r, int cur, 
                           uint32_t addr) {
    while (r) {
      int i1 = (addr >> r->_bitshift) & (r->_n - 1);
      const Child &c = r->_children[i1];
      if (c.key)
       cur = c.key;
      r = c.child;
    }
    return cur;
  }
\end{lstlisting}
\subsubsection{Route Vector for RadixIPLookup}
The Click library files provide an implementation of a generic Vector. A brief explanation of the use of the vector class in RadixIPLookup is described here. The RadixIPLookup Element uses a vector of type IPRoute. IPRoute is a quintuple consisting of \emph{address}, \emph{mask}, \emph{gateway}, \emph{port} and \emph{extra}. \emph{Address} is the IP address of the route. \emph{Mask} is the subnet mask for that route. \emph{Gateway} is the destination to which a packet matching an address/mask will be routed to through the port. \emph{Extra} is used to recycle previously used but currently unused space within the vector. It is used to maintain a list of unused entries in the vector. We call this list the \emph{freelist}. If the index in the vector is currently in use extra is set to $-1$, otherwise it stores the index of the next entry in the freelist. If an entry in the vector is deleted, the index of that entry is added to a freelist. Table \ref{tbl:vector} represents a sample vector.

\begin{table}[float=tph]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline index & addr & mask & port & gw & extra\\ 
\hline 0 & 0.0.0.1 & 255.255.255.0 & 1& 22.34.198.3 & 1\\ 
\hline 1 & 1.1.0.1 & 255.255.255.0 & 1& 1.34.198.3 & 5\\
\hline 2 & 2.2.2.1 & 255.255.255.0 & 1& 2.34.198.3 & $-1$\\
\hline 3 & 1.0.0.1 & 255.255.255.0 & 1& 9.34.198.3 & $-1$\\
\hline 4 & 4.0.0.1 & 255.255.255.0 & 1& 8.34.198.3 & $-1$\\
\hline 5 & 4.0.0.1 & 255.255.255.0 & 1& 133.34.198.3 & 3\\
\hline
\end{tabular}
\end{center}
\caption{Sample vector with routes}
\label{tbl:vector}
\end{table}

\begin{figure}[tph]
\begin{center}
\begin{tabular}{|p{2.5in} c|}
\hline
\textbf{State 1}: Initial state of the vector. The freelist reads \{\verb$_vfree =0$, $0\rightarrow1\rightarrow5\rightarrow3$\}. & \raisebox{2ex - \height}{\includegraphics[width=2in,scale=0.4]{../images/diagrams/freelist1.eps}} \\
\hline
\textbf{State 2}: A route is inserted. The index 0 is reused. The freelist reads \{\verb$_vfree =1$, $1\rightarrow5\rightarrow3$\}.& \raisebox{2ex - \height}{\includegraphics[width=2in,scale=0.4]{../images/diagrams/freelist2.eps}} \\
\hline		
\textbf{State 3}: Another route is inserted. The freelist now reads \{\verb$_vfree =5$, $5\rightarrow3$\}.& \raisebox{2ex - \height}{\includegraphics[width=2in,scale=0.4]{../images/diagrams/freelist3.eps}} \\
\hline		
\textbf{State 4}: Route located at index 2 is removed from the vector. Index 2 is added to the freelist. The freelist now reads \{\verb$_vfree =2$, $2\rightarrow5\rightarrow3$\}.& \raisebox{2ex - \height}{\includegraphics[width=2in,scale=0.4]{../images/diagrams/freelist4.eps}} \\
\hline
\end{tabular}
\cprotect\caption{Example illustrating the working of the freelist in the vector. \verb+_vfree+ stores the starting index of the freelist. A value of $-1$ in the extra field indicates that the entry is in use and hence not a part of the freelist. The address, gateway, mask and port fields are not shown.}
\label{fig:freelisteg}
\end{center}
\end{figure}


The use of \emph{extra} is illustrated with the help of an example in Figure \ref{fig:freelisteg} for the sample vector shown in Table \ref{tbl:vector}. Reads and deletions are O(1). Insertions are O(1) amortized over the cost of many insertions. This is because the vector is dynamically resized when it is full.\\

The vector is initialized to hold a specific number of entries (\verb+_capacity+). The number of entries currently in use in the vector is \verb+_n+. The \verb+push_back()+ function adds an entry to the vector. This increments \verb$_n$ by one. If the vector is full (i.e. \verb+_n+ $\ge$ \verb+_capacity+), it is resized to double the value of its previous capacity with a call to \verb+reserve_and_push_back()+.

\section{The Problem}
\label{sec:problem}
Our goal is to improve lookup performance by allowing for multi-thread
access to Click elements. In this section we look at existing problems
which hinder safe multi-thread access.\\

This section is divided into Updater-Updater Conflicts and
Reader-Updater Conflicts. The Updater-Updater Conflicts section
describes how multiple updaters can cause race conditions which
corrupt the state of shared data structures. The Reader-Updater
Conflicts section describes how a reader racing with an updater might
read stale or invalid data.

\subsection{Correctness}
\label{sec:correctness}
Before analyzing race conditions, we define what we deem acceptable results for concurrent reads and updates.
\begin{enumerate}
\item Concurrent updaters exhibit \emph{sequentially consistent} behavior \cite{seqconsistency}.\\ The result of concurrent updates is as if the updates were applied in some sequential order.
\item Concurrent reads are \emph{eventually consistent} \cite{eventualconsistency}.\\ Concurrent readers must receive some valid route. Specifically, if a lookup (read) for an IP address \emph{A} returns a route \emph{R}, then for some duration of time, R was a valid route for IP address \emph{A}. Note that R need not be the most up to date route for IP address \emph{A}---address lookups can result in receiving stale routes for short time periods. However, address lookups (reads) must eventually return the most up to date route.

\end{enumerate} 
\subsection{Updater-Updater Conflicts}
We now deal with race conditions and conflicts arising due to multiple updaters running concurrently using the same instance of RadixIPLookup. When multiple threads use the same instance of RadixIPLookup, they access the same data structures within RadixIPLookup.
\paragraph{Radix Tree}
The radix tree suffers from Update-Update conflicts. If multiple updaters try to modify the same region of the tree, there is a race in \verb+change()+ for the assignment of the child pointer. This could cause some of the updates to be lost and also cause a memory leak. The size of the memory leak can be equal to the depth of the tree times the size of a radix node.\\

The race described above is illustrated with the help of Listing \ref{change}.
\begin{lstlisting} [label =change, caption={A snippet of the \texttt{change()} function where a race can cause a memory leak. This is illustrated in Figure \ref{fig:raceinchange}}.,float=tph]
  int
  RadixIPLookup::Radix::change(uint32_t addr, uint32_t mask,
                                          int key, bool set)
  {
    ...
    if (mask & ((1U << _bitshift) - 1)) {
      if (!_children[i1].child
      && (_children[i1].child = make_radix(_bitshift - 4, 16)))
    ...
    ...
  }
\end{lstlisting}

\begin{figure}[tph]
\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Thread 1} & \textbf{Thread 2}\\
\hline
\verb$!_children[i1].child$ evaluates to true. & \verb$!_children[i1].child$ evaluates to true.\\
\hline
Calls \verb$make_radix()$. &  Calls \verb$make_radix()$.\\
\hline
\begin{tabular}{p{1.5in}}  
\raisebox{2ex - \height}{\includegraphics[scale=0.4]{../images/diagrams/radix1.eps}}\\
\texttt{make\_radix()} returns pointer ptr1 to a newly created radix node.\\
\end{tabular} 
& 
\begin{tabular}{p{1.5in}}
  \raisebox{2ex - \height}{\includegraphics[scale=0.4]{../images/diagrams/radix2.eps}}\\
  \texttt{make\_radix()} returns pointer ptr2  to a newly created radix node.\\
\end{tabular}\\
\hline
\begin{tabular}{p{1.8in}}  
\raisebox{2ex - \height}{\includegraphics[scale=0.4]{../images/diagrams/radix1race.eps}}\\\\
 \texttt{\_children[i1].child = make\_radix(\_bitshift - 4, 16)}.\\
The node is added to the radix tree.\\
\end{tabular} 
& 
\begin{tabular}{p{1.8in}}
  \raisebox{2ex - \height}{\includegraphics[scale=0.4]{../images/diagrams/radix2race.eps}}\\\\
  \texttt{\_children[i1].child = make\_radix(\_bitshift - 4, 16)}.\\
  The node is added to the tree. We have a memory leak since the older node is now orphaned. Additionally, Thread 1's update is lost. \\
\end{tabular}\\
\hline
\end{tabular}
\cprotect\caption{Example illustrating the race in the \verb$change()$ function. The \verb$change()$ function is used in the insertion of new radix nodes. Two concurrent threads insert a radix node into the tree leading to a memory leak.}
\label{fig:raceinchange}		
\end{center}
\end{figure}

The \verb+change()+ function in Listing \ref{change} is used to create a radix node for a route with a specific address and mask. The key passed as a parameter is the index into the vector where the route is stored. Two or more concurrently executing threads create race conditions which can lead to memory leaks. This is explained pictorially in Figure \ref{fig:raceinchange}. Looking at the assignment, we see that if there are multiple threads executing \verb$change()$, there is a race for the assignment of the radix node. Multiple threads may call \verb+make_radix()+, but only one pointer which is returned by \verb+make_radix()+ is assigned. 
\paragraph{Vector}
When there are multiple updaters there is contention for acquiring an index into the vector. Many updaters trying to add a route might receive the same index value. If multiple updaters are given the same value of the index, there can be an inconsistency in the state of the vector. For example, the size of the vector might be stored wrongly or the freelist might be updated wrongly. Since the vector is resized dynamically, we could lose updates and have memory leaks.\\

We explain the conflicts mentioned above with examples using the \verb+add_route()+ function shown in  Listing \ref{addroute}.
\begin{lstlisting}[caption = The add\_route function, label=addroute,float=tph]
  int
  RadixIPLookup::add_route(const IPRoute &route, 
                           bool set, 
                           IPRoute *old_route, 
                           ErrorHandler *)
  {
    int found = (_vfree < 0 ? _v.size() : _vfree), last_key;
    if (route.mask) {
      uint32_t addr = ntohl(route.addr.addr());
      uint32_t mask = ntohl(route.mask.addr());
      last_key = _radix->change(addr, mask, found + 1, set);
    } else {
      last_key = _default_key;
      if (!last_key || set)
        _default_key = found + 1;
    }
    if (last_key && old_route)
      *old_route = _v[last_key - 1];
    if (last_key && !set)
      return -EEXIST;
    if (found == _v.size())
      _v.push_back(route);
    else {
      _vfree = _v[found].extra;
      _v[found] = route;
    }
    _v[found].extra = -1;
    if (last_key) {
      _v[last_key - 1].extra = _vfree;
      _vfree = last_key - 1;
    }
    return 0;
  }
\end{lstlisting}
We consider two cases, one in which the concurrently executing threads retrieve an index from the freelist, the other in which the concurrently executing threads make a call to \verb+push_back()+ on the vector.\\

\subsubsection{Case 1: Concurrent updaters use the freelist.}
We now illustrate how a race condition can corrupt the state of the vector. The same example is illustrated pictorially in Figure \ref{fig:raceinfreelist}.
Assume the freelist is as follows before the execution of this code sequence:\\
Initially the freelist reads \texttt{\{\_vfree =3, $3\rightarrow1 \rightarrow 5$\}}. Consider the following sequence of events:
\begin{enumerate}
\item Thread 1 calls \texttt{add\_route(Y,1,NULL,NULL)} with \texttt{Y.addr = A}, \texttt{Y.mask = M}.
\item Thread 2 calls \texttt{add\_route(Y,1,NULL,NULL)} with \texttt{Y.addr = A}, \texttt{Y.mask = M}.
\item Updater 1 and Updater 2 execute line 4 concurrently and recieve the same value of \emph{found}. Let us assume that \emph{found} is some key within the vector and is not equal to \emph{\_v.size()}. Updater 1 executes line 7 first. This updates the key in the radix tree to be the value \emph{found} which is $3$ according to our example.
\item Updater 2 executes line 7. The value of the key is set to \emph{found} which is $3$ since the freelist has not changed. (See Figure \ref{fig:raceinfreelist}).
\item Updater 1 executes the remaining code in \verb+add_route()+ including line 27 which sets  \verb+_v[found].extra+ to $-1$ (which means that the index is in use). After the Updater 1 has finished executing add\_route(), the freelist reads\\
\texttt{\{\_vfree =1, $1 \rightarrow 5$\}}.
\item Updater 2 executes the remaining code sequence. It executes line 24 which sets \texttt{\_vfree} to \texttt{\_v[found].extra}. \texttt{\_v[found].extra} has been set to $-1$ by Updater 1. Thus \texttt{\_vfree} is now $-1$.\\
The state of the freelist is now \texttt{\{\_vfree = $-1$, $1 \rightarrow 5$\}}. The correct state is \texttt{\{\_vfree = 1, $1 \rightarrow 5$\}}. If \texttt{\_vfree} is $-1$, it means that there are no more free keys in the vector, which is not true. This sequence brings the vector into an inconsistent state. As a result, the routing table is also in an inconsistent state.\\
\end{enumerate}

\begin{figure}[tph]
\begin{center}

\begin{tabular}{|p{4.6in}|}
\hline
Initial state of the freelist:\\
\includegraphics[scale=0.4]{../images/diagrams/freelistrace1}\\
\hline
\end{tabular}
\begin{tabular}{|p{2.2in}|p{2.2in}|}
\hline
\textbf{Thread 1} & \textbf{Thread 2}\\
\hline
calls
\begin{Verbatim}[baselinestretch=1,fontsize=\small]
add_route(X,1,NULL,NULL) 
\end{Verbatim}
&
calls
\begin{Verbatim}[baselinestretch=1,fontsize=\small]
add_route(Y,1,NULL,NULL)
\end{Verbatim}
\\
\hline
Executes:
\begin{Verbatim}[baselinestretch=1,fontsize=\small]
found= (_vfree < 0 ?
        _v.size():_vfree)
\end{Verbatim}
Obtains a value \texttt{found= \_vfree= 3}
&
Executes:
\begin{Verbatim}[baselinestretch=1,fontsize=\small]
found= (_vfree < 0 ?
        _v.size():_vfree)
\end{Verbatim}
Obtains a value \texttt{found= \_vfree= 3}\\
\hline
\begin{tabular}{p{1.8in}}
Executes:
...
\begin{Verbatim}[baselinestretch=1,fontsize=\small]
_v[found].extra = -1;
\end{Verbatim}
As a result, the freelist is updated as shown in the figure. \\\\
\hline
\raisebox{2ex - \height}{\includegraphics[scale=0.4]{../images/diagrams/freelistrace2}}\\\\
\end{tabular}
&
Thread is blocked or paused or context-switched.\\
\hline
Thread exits \verb+add_route()+.
&
\begin{tabular}{p{1.8in}}
Executes:\\
\verb$_vfree = _v[found].extra$.\\
\verb$_v[found].extra$ was set to $-1$ by thread 1. So \verb$_vfree$ is $-1$.
Freelist is now updated as shown in the figure. This is wrong since \texttt{\_vfree} is -1 although there are unused indices in the freelist. A value of $-1$ for \texttt{\_vfree} indicates that the freelist is empty which is untrue.\\\\
\hline
\raisebox{2ex - \height}{\includegraphics[scale=0.4]{../images/diagrams/freelistrace3}}\\\\
\end{tabular}\\
\hline
\end{tabular}
\cprotect\caption{The example illustrates how two concurrent updaters can corrupt the sate of the freelist if they call \texttt{add\_route()}. The initial state of the freelist is as shown in the figure. The first parameter is the route, the second parameter asks the function to use the route supplied even if a route previously existed for that IP address. The third parameter \emph{old\_route} retrieves a preexisting route for that IP if it is not set to NULL. The fourth parameter is the error handler.}
\label{fig:raceinfreelist}
\end{center}		
\end{figure}

\subsubsection{Case 2: Concurrent updaters call \texttt{push\_back()}. }
Now consider the case when \texttt{\_v.push\_back()} is executed concurrently. For this we will look at the Click code from vector.cc in Listing \ref{pushback}.
\begin{lstlisting}[caption = The push\_back() function, label=pushback]
  template <class T> inline void
  Vector<T>::push_back(const T& x)
  {
    if (_n < _capacity) {
      new(velt(_n)) T(x);
      ++_n;
    } else
      reserve_and_push_back(RESERVE_GROW, &x);
  }
\end{lstlisting}
We first consider the case when there is enough space in the vector. Updater 1 and Updater 2 use the same value of \verb+_n+. The updater which wins the race installs its key at position i, and the other update is lost.
We now consider the case when there is not enough space in the vector. The expression \verb$_n < _capacity$ evaluates to false. Some of the race conditions here could bring the vector to an inconsistent state or cause memory leaks. They are better understood with the help of the graphic example in Figure \ref{fig:raceinreserve} and the \texttt{reserve\_and\_push\_back()} function shown in Listing \ref{reserveandpushback}. The assignment of \verb+new_l+ in line 15 of Listing \ref{reserveandpushback} is a race condition which can lead to a memory leak. The size of the leak is equal to the size of \verb+new_l+. The other consequence is that one update will be lost. If both the updaters call \texttt{CLICK\_LFREE()} in line 14, we could have a segmentation fault.

\begin{figure}[tph]
\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Thread 1} & \textbf{Thread 2}\\
\hline
Calls \verb$reserve_and_pushback()$. &  Calls \verb$reserve_and_pushback()$.\\
\hline
\begin{tabular}{p{1.5in}}
\\  
\raisebox{2ex - \height}{\includegraphics[scale=0.3]{../images/diagrams/vectorptr1}}\\
\verb$CLICK_LALLOC()$ returns pointer ptr1 to a newly allocated vector.\\
\end{tabular} 
& 
\begin{tabular}{p{1.5in}}
  \\
  \raisebox{2ex - \height}{\includegraphics[scale=0.3]{../images/diagrams/vectorptr2}}\\
  \verb$CLICK_LALLOC()$ returns pointer ptr2 to a newly allocated vector.\\
\end{tabular}\\
\hline
\begin{tabular}{p{1.8in}}  
\raisebox{2ex - \height}{\includegraphics[scale=0.3]{../images/diagrams/vectorrace1}}\\\\
 \verb$new_l = ptr1$.\\
The node is added to the radix tree.\\
\end{tabular} 
& 
\begin{tabular}{p{1.8in}}
  \\
  \raisebox{2ex - \height}{\includegraphics[scale=0.3]{../images/diagrams/vectorrace2}}\\\\
  \verb$new_l = ptr2$.\\
  We have a memory leak since the older node is now orphaned.\\
\end{tabular}\\
\hline

\begin{tabular}{p{1.8in}}  
\raisebox{2ex - \height}{\includegraphics[scale=0.3]{../images/diagrams/vectorrace3}}\\\\
 \verb$CLICK_LFREE(_l, ...)$.\\
 The pointer to the old vector is freed.\\
  \raisebox{2ex - \height}{\includegraphics[scale=0.3]{../images/diagrams/vectorrace4}}\\\\
\end{tabular} 
& 
\begin{tabular}{p{1.8in}}
  \\
  \raisebox{2ex - \height}{\includegraphics[scale=0.3]{../images/diagrams/vectorrace4}}\\\\
\verb$CLICK_LFREE(_l, ...)$.\\  
   \raisebox{2ex - \height}{\includegraphics[scale=0.3]{../images/diagrams/vectorrace5}}\\\\
  Potential failure or memory corruption since the thread attempts to free a pointer which is already freed.\\
\end{tabular}\\
\hline
\end{tabular}
\cprotect\caption{Example illustrating the race during a dynamic resize of the vector.}
\label{fig:raceinreserve}
\end{center}		
\end{figure}


\begin{lstlisting}[caption= reserve\_and\_push\_back(), label =reserveandpushback,float=tph]
  Vector<T>::reserve_and_push_back(size_type want, 
                                  const T *push_x)
  {
    if (want < 0)
      want = (_capacity > 0 ? _capacity * 2 : 4);
    if (want > _capacity) {
      T *new_l = (T *) CLICK_LALLOC(sizeof(T) * want);
      if (!new_l)
        return false;
      for (size_type i = 0; i < _n; i++) {
        new(velt(new_l, i)) T(_l[i]);
        _l[i].~T();
      }
      CLICK_LFREE(_l, sizeof(T) * _capacity);
      _l = new_l;
      _capacity = want;
    }
    if (unlikely(push_x))
      push_back(*push_x);
    return true;
  }
\end{lstlisting}
\subsubsection{Verifying conflicts in Click}
\label{sec:uuconflicts}
 Click scripts were used to verify existing race conditions for checking correctness of our solutions. New elements were created and run in multi-threaded mode. The \emph{StaticThreadSched()} Element was used to specify the thread on which the Element must be run. Listing \ref{lst:uuconflict} facilitates a clearer picture of the use of \emph{StaticThreadSched()} for this case. We created Click elements called BashRadixIPLookup and PoundRadixIPLookup which repeatedly add and delete the same route with the same address but different values of the port. This is illustrated in Table \ref{tbl:uuverify}. The parameters for \texttt{add\_route()} mentioned in the listing are simplified. Both BashRadixIPLookup and PoundRadixIPLookup run concurrently on the same instance of RadixIPLookup. This concurrency is ensured by using the \emph{StaticThreadSched()} Element in the script.
\begin{lstlisting}[float=tph, caption=Click script for verifying updater-updater conflicts.,label=lst:uuconflict]
Idle
 -> r :: RadixIPLookup
 -> Idle;

writer0 :: PoundRadixIPLookup(r);
writer1 :: BashRadixIPLookup(r);

StaticThreadSched(
        writer0 0,
        writer1 1,
);

DriverManager(wait 3, print r.table, stop);

\end{lstlisting}
\begin{table}
\begin{center}
\begin{tabular}{|p{2.2in}|p{2.2in}|}
\hline
\begin{lstlisting}[label=lst:uubashradixiplookup]
bool
BashRadixIPLookup::
run_task(Task *) {
  ...
  for( i=0; i<10000, i++) {
    add_route (A,X);
    delete_route (A,X);
  }
 ...
}
\end{lstlisting}
&
\begin{lstlisting}[label=lst:uupoundradixiplookup]
bool
PoundRadixIPLookup::
run_task(Task *){
  ...
  for(i=0;i<10000,i++){
    add_route (B, Y);
    delete_route (B,Y);
  }
  ...
}
\end{lstlisting}\\
\hline
\end{tabular}
\end{center}
\caption{The \texttt{run\_task()} methods of BashRadixIPLookup and PoundRadixIPLookup.}
\label{tbl:uuverify}
\end{table}

The tests resulted in segmentation faults and assertion failures, which did not occur if locks were acquired during updates.
\subsection{Reader-Updater Conflicts}
The conflicts in the RadixIPLookup Element occur primarily in two data structures: the vector used to store the routes, and the radix tree which stores a key indexing into the vector. This section describes the conflicts which occur in each of these data structures.\\

\subsubsection{Reader-Updater conflicts in the Vector}
We look at reader-updater conflicts which involve read delete races, and discuss their consequences.
The race conditions are described in the context of the \verb+add_route()+ function which adds a route
to the radix tree. The way an index is obtained for insertion is
summarized using the pseudo-code in Listing \ref{lst:pseudoaddroute}.
\begin{lstlisting}[caption = Pseudo-code for acquiring an index in \texttt{add\_route()}, label=lst:pseudoaddroute]
  if(free_list is not empty) {
    index = free_list; free_list = free_list->next;
    _v[free_list.index] = val;
  } else
    _v.push_back(val);
\end{lstlisting}
We can observe at least two inadmissible conflicts which can be caused
by race conditions. Firstly, if two updaters and a reader run
concurrently, the reader might get an index into an entry which has
been freed and reused. The reader may end up receiving a route which
is incorrect for the IP address in question. Figure
\ref{race_readerfigure} illustrates this race. BashRadixIPLookup and
PoundRadixIPLookup are two updaters which update the routing table by
adding a route. ReadRadixIPLookup performs a lookup for IP address
A. There is a chance that the lookup returns a route which was never a
valid route for IP address A. This does not observe the correctness
policy described in Section \ref{sec:correctness}. Secondly, a reader
might access invalid memory in the presence of a concurrent
updater. One must recall that the \verb+push_back()+ operation
dynamically resizes the vector if necessary. If an updater initiates a
dynamic resize operation amidst a concurrent reader, the reader might
access invalid memory.

\subsubsection{Reader-Updater Conflicts in the Radix Tree}
This section describes some of the reader-updater conflicts which
exist within the radix tree and justifies why they are not a source
for concern.\\

We will now look at a race condition which can occur in the radix tree
when a reader and an updater execute concurrently. Consider a case
where we have a reader and an updater running concurrently. The
updater deletes route X which holds information for IP address A, the reader performs a lookup on the very
route A.\\
\begin{figure}[tph]
\begin{center}
\includegraphics[scale=0.6]{../images/diagrams/race1eps.eps}
\end{center}
\caption{Race condition with a concurrent reader and updater}
\label{race1figure}
\end{figure}
We have two threads one running: an updater thread trying to remove a
route X from the routing table, and a reader thread performing a
lookup for IP address A. The reader might receive stale information
for A. This happens if the reader acquires a reference to the radix
node containing Route X before the pointer is removed from the
tree. The race is illustrated with the help of the sequence diagram in
\ref{race1figure}. However, readers which execute after the updater
has finished will not receive stale data for Route X. These results are acceptable according
to our correctness requirement specified in \ref{sec:correctness}.
This is justified by the argument that the reader received
a stale route: it was once valid for the given address, and the
ultimate price paid is the price of one wrong lookup, which as we said above, is acceptable by our policy.
\begin{lstlisting}[caption= The remove\_route() function, label=removeroute,float=tph]
  int
  RadixIPLookup::remove_route(const IPRoute& route, 
                              IPRoute* old_route, 
                              ErrorHandler*)
  {
    int last_key;
    if (route.mask) {
      uint32_t addr = ntohl(route.addr.addr());
      uint32_t mask = ntohl(route.mask.addr());
      // NB: this will never actually make changes
      last_key = _radix->change(addr, mask, 0, false);
    } else
      last_key = _default_key;
    if (last_key && old_route)
      *old_route = _v[last_key - 1];
    if (!last_key || !route.match(_v[last_key - 1]))
      return -ENOENT;
    _v[last_key - 1].extra = _vfree;
    _vfree = last_key - 1;
    if (route.mask) {
      uint32_t addr = ntohl(route.addr.addr());
      uint32_t mask = ntohl(route.mask.addr());
      (void) _radix->change(addr, mask, 0, true);
    } else
      _default_key = 0;
    return 0;
  }
\end{lstlisting}

\subsubsection{Verifying conflicts in Click}
Some of the conflicts mentioned in Section were verified by running Click scripts. Our approach for verifying reader-updater conflicts is the similar to the one mentioned in Section \ref{sec:uuconflicts}. Click elements BashRadixIPLookup, PoundRadixIPLookup repeatedly update the routing table. ReadRadixIPLookup repeatedly performs a route lookup. Table \ref{tbl:ruverify} shows how these elements are used to verify conflicts. ReadRadixIPLookup, PoundRadixIPLookup and BashRadixIPLookup run concurrently on an instance of RadixIPLookup. A and B are the addresses, X and Y are the values of the port. The least-common ancestor of A and B is the root node. Locks are acquired during updates since we want to verify reader-updater conflicts as opposed to updater-updater conflicts. All three of these are run concurrently on the same instance of RadixIPLookup. The race is illustrated with the help of a sequence diagram in Figure \ref{race_readerfigure}.\\

A thread-safe implementation will have us expect that we always get X or the default route. A default route is returned if there was no prefix matching the route being looked up in the tree. But we see that the lookup sometimes returns Y, which is incorrect.

\begin{figure}[tph]
\fbox{
\includegraphics[scale=0.6]{../images/diagrams/race_reader.pdf}
}
\caption{Race condition with a concurrent reader and updater}
\label{race_readerfigure}
\end{figure}

\begin{table}[float=tph]
\begin{center}
\begin{tabular}{|p{2.2in}|p{2.2in}|}
\hline
\begin{lstlisting}[label=lst:uubashradixiplookup]
bool
BashRadixIPLookup::
run_task(Task *) {
  ...
  for( i=0; i<10000, i++) {
    add_route (A,X);
    delete_route (A,X);
  }
 ...
}
\end{lstlisting}
&
\begin{lstlisting}
bool
PoundRadixIPLookup::
run_task(Task *){
  ...
  for(i=0;i<10000,i++){
    add_route (B, Y);
    delete_route (B,Y);
  }
  ...
}
\end{lstlisting}\\
\hline
\end{tabular}
\begin{tabular}{|p{4.55in}|}
\begin{lstlisting}
bool
ReadRadixIPLookup::
run_task(Task *){
  ...
  for(i=0;i<10000,i++){
    lookup_route (A);
  }
  ...
}
\end{lstlisting}\\
\hline
\end{tabular}
\end{center}
\caption{The \texttt{run\_task()} methods of BashRadixIPLookup, PoundRadixIPLookup and ReadRadixIPLookup.}
\label{tbl:ruverify}
\end{table}


\pagebreak
\section{The Solution}
\label{sec:solution}
The race conditions described in the previous sections occur due to threads accessing shared data structures. RCU is known to be an efficient synchronization technique for read-heavy workloads. We first describe our solution using RCU. We then describe our solution using a reader-writer lock.

\subsection{Read-Copy Update}

\label{sec:rcu}


\subsubsection{Conceptual Overview}
RCU allows readers to proceed concurrently with an updater. The updater can \textbf{remove} shared data structures, but is not allowed to \textbf{delete} it. Multiple copies of shared data structures are maintained. The old copies are deleted when they are no longer being used. A thread reaches a quiescent state when it releases references to shared data structures. A grace period is reached when all threads have undergone at least one quiescent state. It is safe to delete stale copies (reclaim memory) during a grace period. Figure \ref{fig:rcuexp1} illustrates this concept.

\begin{figure}[float=tph]
\begin{center}
\includegraphics[scale=0.4]{../images/diagrams/rcuexp0}
\caption{Thread 1 and Thread 2 begin concurrent execution. A, B and C represent read-side critical sections which read the pointer \emph{x}. Z is an update-side critical section which updates the pointer \emph{x} by deleting the old pointer. A and B access \emph{x} when Z is in progress. We allow Z to remove the old pointer but we do not allow Z to \textbf{free} it. C reads the value of \emph{x} \textbf{after} Task Z has finished; it is not dependent on the older value of \emph{x}. It is safe to delete the older copy of \emph{x} after A and B have completed.}
\label{fig:rcuexp1} 
\end{center}
\end{figure}

\pagebreak

\subsubsection{RCU Using Timers}
\label{sec:rcutimers}
IP lookups are read-side critical sections. An IP lookup reads the routing table which is a shared data structure. IP lookups using RadixIPLookup finish within $1000$ ns on modern commodity machines \cite{lookuptime}. A fixed time interval $\ge 1000$ ns can be a grace period to free stale data-structures. An interval of $1000$ ns leads to very frequent reclamations. A very wide time interval can lead to infrequent reclamations, and consequently, a larger memory footprint. So a convenient interval of $500$ ms is chosen.\\ 

Figure \ref{fig:rcutimer} explains the working of the timer mechanism. Our implementation involved using the Click Timer class to schedule reclamations. Click provides a Timer class which calls functions at specified time intervals. We use this to schedule a callback which reclaims freed data at a time interval of $500$ ms.
\begin{figure}[float=tph]
\begin{center}
\includegraphics[scale=0.28]{../images/diagrams/rcutimer}
\caption{ Three threads run concurrently. In the first 500 ms interval, an updater changes a shared data structure. Readers which have started before the update completes recieve the older value of \emph{x}. Readers which begin after the updater completes recieve the newer value. In each interval, stale data from the intervals prior to the previous interval can be reclaimed. It is safe to reclaim \emph{x} during or after the third interval and safe to reclaim \emph{z} during or after the fourth interval. Reclamations are highlighted in gray.
}
\label{fig:rcutimer}
\end{center}
\end{figure}

\pagebreak
\subsubsection{RCU Using The Click Scheduling Loop}
\label{sec:rcuschedloop}

The timer expiry as a quiescent state described in Section \ref{sec:rcutimers} works well for RadixIPLookup since each lookup completes within a fixed time interval. For other elements in Click, it may not be possible to determine the duration of a read-side critical section. We wish to detect quiescent states for all other userlevel elements without having to know how long it takes for a reader task to complete. The scheme described here does not require that the reader task be of a specific duration. It does require that the reader task eventually terminate. The approach involves using the scheduling loop in Click to detect quiescent states. 

\paragraph{The Click RouterThread Driver Loop}
A \emph{task} in Click is something that requires CPU time or packet processing time. A task can involve performing an IP lookup, encapsulating a packet, etc. Click has a class called \emph{RouterThread} which is responsible for running tasks. When Click is allowed to run with say $N$ threads, $N$ instances of \emph{RouterThread} are created. Each \emph{RouterThread} instance runs a \verb+driver()+ function which runs tasks scheduled by Click elements in a loop. We will refer to this loop as the ``driver loop''. After running a specified number of tasks, control returns to the driver loop. A highly abridged version of the driver loop is shown in Algorithm \ref{alg:driverloop}.


\begin{algorithm}[float=tph]
\begin{algorithmic}
{
\STATE $forall \mbox{ threads }t_i:$
\WHILE {$\exists \mbox{ a task for thread }t_i$} 
\STATE Run a task.
\ENDWHILE
}
\caption{The RouterThread Driver Loop}
\label{alg:driverloop}
\end{algorithmic}
\end{algorithm}

\paragraph{Grace Periods and Reclamation}
We describe our approach with the help of Figure \ref{fig:rcuexp2} and in more formal terms using Algorithm \ref{alg:rcu}. Each \emph{RouterThread} instance maintains a local epoch counter, $le$. The local epoch counter can either be zero or one. The application has a global epoch counter, $GE$. The global epoch counter is not physically stored; we only use it to explain and justify our approach. Initially all local epoch counters are set to one. After a thread completes a task, it resets its local epoch number. When the local epoch counters of all threads are reset, we reach a grace period. (All threads have released references to shared data structures they once held.) At this point, we reclaim stale data structures from the penultimate global epoch, increment the global epoch counter, set all local epoch counters and continue.
\begin{figure}[float=tph]
\begin{center}
\includegraphics[scale=0.36]{../images/diagrams/rcuexp2}
\caption{Thread 1 and Thread 2 execute concurrently. 
The white rectangle indicates that a thread has updated its epoch counter and attempted to reclaim, but was unsuccessful. The black rectangle indicates that a thread has updated its epoch counter and has reclaimed successfully. Following a reclamation, all local epoch counters are reset to 1. 
It is only safe to free \emph{x}  \textbf{after} \emph{global epoch 1} has passed. Task Z of \emph{Thread1} modifies the pointer \emph{x} in \emph{global epoch 0}. Task A and B dereference \emph{x} during \emph{global epochs 0} and \emph{1}. For this reason it is not safe to free \emph{x} in \emph{global epoch 0} or \emph{1}. Task C reads \emph{x} in \emph{global epoch 2}, \textbf{after} the updater has finished. For this reason, it receives the value of \emph{x} updated by task Z. We advance the \emph{global epoch counter} when we reach a grace period, i.e. when all threads cease to hold references to shared data structures.}
\label{fig:rcuexp2}
\end{center}
\end{figure}

\begin{algorithm}[float=tph]
\begin{algorithmic}
{
\STATE Each thread $t_i$ has a local epoch number $le_i$.
\STATE The application has a global epoch number $GE$.\\
\STATE Initially  $GE=0$ and $\forall t_i, le_i=1$. \\
\STATE $forall \mbox{ threads }t_i:$
\WHILE {$\exists \mbox{ a task for thread }t_i$} 
\STATE Run a task.
\STATE set local epoch  $le_i=0$.
\IF {$\forall t_i, le_i=0$}
  \STATE Set $GE=GE+1$.
  \STATE Reclaim stale data from $GE-2$.
  \STATE $\forall t_i$,Set $le_i=1$.
\ENDIF
\ENDWHILE
}
\caption{Algorithm for detecting grace periods.}
\label{alg:rcu}
\end{algorithmic}
\end{algorithm}
The transition between \emph{global epoch 1} and \emph{global epoch 2} in Figure \ref{fig:rcuexp2} is a grace period for updaters which finish in \emph{global epoch 0}. This is because any readers which start in \emph{global epoch 0} have finished by the end of \emph{global epoch 1}. It is safe to reclaim memory in \emph{global epoch 2}. We can extend this observation using Figure \ref{alg:rcu} and say that stale data accumulated in \emph{global epoch N} can be reclaimed during \emph{global epoch N+2}. Note that we cannot do this during \emph{global epoch N+1} since a reader which starts in \emph{global epoch N} might not finish until the end of \emph{global epoch N+1}.\\

In order to facilitate reclamation of stale data accumulated during \emph{global epoch N} in \emph{global epoch N+2}, we maintain two lists: a \emph{reclaim\_now} list and a \emph{reclaim\_later} list. When an updater modifies a shared data structure, we maintain the updated copy as well as the older copy. The older copy is also added to the \emph{reclaim\_later} list. Whenever a grace period is reached:
\begin{enumerate}
\item Reclaim everything in the \emph{reclaim\_now} list. The \emph{reclaim\_now} list becomes empty after this step completes.
\item Swap the \emph{reclaim\_now} list and the \emph{reclaim\_later} list.\\
The \emph{reclaim\_later} list becomes empty and the \emph{reclaim\_now} list has the contents of the previous \emph{reclaim\_later} list after this step completes.
\end{enumerate}

\paragraph{RCU guarantees} 
For our scheme, we can provide the two RCU guarantees expressed in \cite{urcu}. The \emph{Grace Period Guarantee} says that a given read-side critical section cannot extend beyond both sides of a grace period. The \emph{Publication Guarantee} says that readers will see consistent newer versions of the data structures updated by writers. These guarantees are important since they show that readers always access consistent copies of data structures. 
\begin{enumerate}
\item \emph{Grace-Period Guarantee:} This guarantee states that a given read-side critical section cannot extend beyond both sides of a grace period. We now show that we can maintain this as an invariant in our scheme.

\begin{algorithmic}[float=tph]
\STATE Any outstanding tasks in global epoch $GE=N-2$ have finished before the start of global epoch $GE=N$.

\STATE Consider any task which started in $GE = N-2$,

\STATE The local epoch $le_i$ can either be 0 or 1.
\newline
\STATE If $le_i==1$ :
\INDSTATE GE can move to $N-1$ only after all $le_i$ have been reset to 0. 
\INDSTATE $le_i$ is reset to zero only after the task finishes.
\INDSTATE $\therefore$ the task finishes before $GE=N-1$, it finishes before $GE=N$.
\newline
\STATE If $le==0$ :
\INDSTATE $le_i$ is set when the global epoch progresses to $GE = N-1$.
\INDSTATE $le_i$ is reset before GE progresses from N-1 to N.
\INDSTATE $le_i$ can reset only after this task finishes.
\INDSTATE $\therefore$ the task finishes before $GE=N$.
\end{algorithmic}

\item \emph{Publication Guarantee:} 
Although readers proceed concurrently with writers, readers must see either the old version of the data or the newer version. Readers should not see an inconsistent state of the data structure. Readers which begin after an updater has completed should see the newer version. In order to ensure that readers do not see an inconsistent version for RadixIPLookup, variables which reflect the state of the vector such as \verb+_capacity+ and \verb+_n+ are updated only after the pointers are updated. Memory barriers are also used to ensure that these variables reflect their newer values only after memory has been allocated.\\
\end{enumerate}


We observe that our approach is similar to the \emph{fuzzy barrier} approach described by Hart et al. \cite{hart}. In our scheme, the barrier is the condition which checks that all local epoch variables are set to zero. The barrier protects access to the reclamation code. This barrier is fuzzy because the threads resume execution of other tasks if they are unable cross the barrier and reclaim (non-blocking).

\subsubsection{Protecting against Updater-Updater Conflicts}
RCU allows concurrent readers to proceed amidst other readers or a concurrent updater. Concurrent updaters require some form of synchronization. We use locks around update-side critical sections to synchronize concurrent updaters.

\subsubsection{The RCU API for Userlevel Click elements}
 We describe the API for userlevel elements, which can be used by an Element to provide RCU synchronization. This section describes how an Element can use the API to provide RCU synchronization, as well as how the API interfaces with the driver loop in Click.
\paragraph{Requirements for Userlevel elements}
 The ReclaimHook class (Listing \ref{lst:reclaimhook}) has been added to facilitate RCU synchronization for userlevel elements. The framework identifies \emph{when} it is safe to reclaim. It is the responsibility of the userlevel Element to identify mutable data structures, and provide callbacks which correctly reclaim memory. A userlevel Element which seeks to use RCU synchronization using our framework is required to do the following:
\begin{enumerate}
\item The Element should declare a ReclaimHook variable as a member of the class (for example, declare \texttt{ReclaimHook \_reclaimhook}).
\item The Element should provide a \texttt{reclaim()} method as a member function or as a member of a subclass. The instance of the class containing the \texttt{reclaim()} function is the \emph{Reclaimable} object. \\ This function should reclaim stale data from the \texttt{reclaim\_later} list when called, using the mechanism described in Section \ref{sec:rcuschedloop}.
\item The ReclaimHook should be initialized using \texttt{initialize(this)} before any reclamations are scheduled.
\item Reclamations can be scheduled or unscheduled using the \texttt{schedule()} or \\\texttt{unschedule()} methods respectively.
\item Update-side critical sections must be protected using locks (unless a single updater thread is used).
\end{enumerate}
 
\begin{lstlisting}[float=tph, caption=The ReclaimHook class which allows elements to schedule reclamations., label=lst:reclaimhook]
class ReclaimHook:public Hook {

  public:
    inline ReclaimHook(Reclaimable *);
    inline ~ReclaimHook() {}

    inline void initialize(Element *);
    inline bool scheduled()  { return _is_scheduled; }
    inline void schedule()   { _is_scheduled = true; } 
    inline void unschedule() { _is_scheduled = false; }
    inline void fire();

  private:
    Element *_owner;
    void * _thunk;
    bool _is_scheduled;    
};

inline
ReclaimHook::ReclaimHook(Reclaimable *r)
    :_thunk(r), _is_scheduled(false){

}

void
ReclaimHook::initialize(Element *owner) {
    assert(owner);
    Router *router = owner->router();    
    router->master()->add_reclaim_hook(this);
    _owner = owner;
}

void
ReclaimHook::fire() {
  if(_thunk) {
    ((Reclaimable *)_thunk)->reclaim();
  }
}
\end{lstlisting}

\paragraph{API Usage in the Driver Loop}
An element using our API registers a callback function (\texttt{reclaim()}), which reclaims stale data belonging to that Element, as described previously. When a grace period is reached, it is safe to reclaim stale data accumulated two epochs prior to the current epoch, as discussed in Section \ref{sec:rcuschedloop}. When we reach a grace period, we invoke all registered callbacks using the \texttt{ReclaimHook::fire()} method. This is illustrated in Listing \ref{lst:graceperiod}. We can see that after a grace period, all elements which had scheduled reclamations have had a chance to reclaim stale data.  
\begin{lstlisting}[float=pht, caption=Invoking callbacks for reclamations using the API.,label=lst:graceperiod, lineskip={-0.4pt}]
void 
Master::try_reclaim()
{
    // multiple threads call try_reclaim at the same time,
    // but only one should excecute it.
    bool got_lock = _try_reclaim_lock.attempt();
    if(got_lock == false)
	return;

    bool reclaim = true;
    RouterThread *t;
    // The condition for a grace period is that for all threads:
    // either the thread has witnessed a quiescent state
    // ( i.e.  _thread_epoch_counts[thread_id] == 0 )
    // or the thread is blocked.
    int n = nthreads();
    for(int i = 0; i < n ; i++) {
	t = thread(i);
	int state = t->thread_state();
	if(_thread_epoch_counts[i] ==1 && 
	   (state == RouterThread::S_RUNTIMER ||
	    state == RouterThread::S_RUNSELECT ||
	    state == RouterThread::S_RUNSIGNAL ||
	    state == RouterThread::S_RUNSELECT ||
	    state == RouterThread::S_RUNTASK)) {
	    reclaim = false;
	    break;
	}
    }

    if(reclaim){
	// reclaim memory
	for(int i = 0; i < _reclaim_hooks.size(); i++) {
	    if(_reclaim_hooks[i]->scheduled())
		_reclaim_hooks[i]->fire();
	}
	// reset the _thread_epoch_count variables for each thread.
	asm volatile ("" : : : "memory");
	for(int i = 0; i < n; i++) {
	    t = thread(i);
	    _thread_epoch_counts[i]= 1;
	}
    }
    _try_reclaim_lock.release();
}
\end{lstlisting}

\subsection{Reader-Writer Lock}
We have compared the performance of the RCU approach with that of a reader-writer lock. We chose a reader-writer lock since the typical workload for a router is read intensive with few updates. In the absence of an updater, all readers can access shared data without contending for the lock.\\

Our approach involves locking around the entire read-side or update-side critical section. This is illustrated in the \verb+add_route()+ function shown in Listing \ref{addroutereaderwriterlocklisting}, where a lock is acquired around the entire update-side critical section. Similar locking is used around the read-side critical section in \verb+lookup_route()+ (Listing \ref{lookuproutereaderwriterlocklisting}).
\begin{lstlisting}[caption = Reader-writer lock usage in lookup\_route(), label=lookuproutereaderwriterlocklisting,float=hpt]
int RadixIPLookup107::lookup_route(IPAddress addr, 
                                   IPAddress &gw) const
{  
    _lock.acquire_read();
    ...
    read side critical section
    ...
    _lock.release_read();
    return port;
}
\end{lstlisting}
\begin{lstlisting}[caption = Reader-writer lock usage in add\_route(), label=addroutereaderwriterlocklisting,float=hpt]
int
RadixIPLookup107::add_route(const IPRoute &route, 
                            bool set, 
                            IPRoute *old_route, 
                            ErrorHandler *)
{
  _lock.acquire_write();
  ...
  write-side crtical section
  ...
  _lock.release_write();
  return 0;
}
\end{lstlisting}
\pagebreak  
\subsection{Performance Hypothesis}
\label{sec:perfhypothesis}
RCU is known to work best for a reader-heavy workload with a small number of updaters. Our claim is that the RCU mechanism for click outlined above has almost zero reader-side overhead. Readers being lock-free and wait-free, we expect RCU performance to be comparable to the original version when there are only readers.\\

Updaters acquire a lock, so there is some performance penalty in the presence of updaters. We expect the RCU performance for an update-intensive workload to be comparable to that of a Reader-Writer lock.

\section{Performance Evaluation}
\label{sec:perfeval}
\subsection{Experimental Setup}
We analyzed the performance of our solutions on the machine whose configuration is shown in Table \ref{tbl:machinemac}.
We used the CPU time reported by system utility \emph{/usr/bin/time} to measure performance.

Macrobenchmarks in Section \ref{sec:macrobenchmarks} evaluate the performance of RCU over a routing table consisting of roughly 167,000 routes. Microbenchmarks in Section \ref{sec:microbenchmarks} evaluate the performance of RCU with a over a small range of IP addresses.

\begin{table}
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline Feature & Value\\
\hline CPU &Intel(R) Core(TM) i7-2600 CPU @ 3.40GHz\\
\hline Number of Cores & 8\\
\hline Operating System & Mac OS X 10.7.2 11C74\\
\hline Cache-line size & 64 bytes\\
\hline L1 cache size (per-CPU) & 32 KB\\
\hline L2 cache size (shared by 2 CPUs) & 256 KB\\
\hline L3 cache size (shared by 8 CPUs)& 8 MB\\
\hline Main memory size & 16 GB\\
\hline
\end{tabular}
\end{center}
\caption{Machine configuration.}
\label{tbl:machinemac}
\end{table}

%% Macro benchmark sub-section
\subsection{Macro-benchmarks}
\label{sec:macrobenchmarks}

The macrobenchmarks are designed to reflect a typical software router
use case. Usually a router will encounter far more read requests (IP
lookups) as compared to write requests (routing table updates) . To
model this we consider pure reader workloads and workloads which are
read intensive with a very low fraction of writes.\\

We used a realistic routing table derived from the routeviews.org
database \cite{routeviews}. This table consists of 167,000 routes. We call this table
the \emph{167k table}. The input set was generated randomly using the
167k router configuration. A reader task consisted of performing
lookups for 100,000 inputs from this input set. An updater task
involves replacing routes for 1000 routes in the input set. Each of
the reader and updater threads execute 128 such tasks in a single run
of the test.\\

The benchmarks involving only readers were run on the RCU version,
the reader-writer lock version and the vanilla version with no
locks. When there were updaters involved in the workload, the benchmark
was run on the RCU version and the reader-writer lock version. We
cannot run the workload with writers on the vanilla version since it
is not thread safe.

\subsubsection{Pure Reader Workload}
 We first look at a workload consisting of only readers. The results
 are shown in Figure \ref{img:macro_vr_0w} and Table
 \ref{tbl:macro_vr_0w}.

\begin{table}[tph]
\begin{center}
\scalebox{0.9} {
\input{macro_vr_0w.tex}
}
\end{center}
\caption{Performance comparison over a workload with increasing number of readers using the 167k routing table. The first three columns show time in seconds. Smaller numbers are better.}
\label{tbl:macro_vr_0w}
\end{table}

\begin{figure}[tph]
\centering
\includegraphics[scale=0.8]{../images/graphs/macro_vr_0w}
\caption[justification=justified]{Performance of increasing number of readers with zero writers using the 167k routing table. Performance of RCU is very close to non-locking performance. Performance of the reader-writer lock degrades linearly.}
\label{img:macro_vr_0w}
\end{figure}

\begin{figure}[tph]
\centering
\includegraphics[scale = 0.7]{../images/graphs/profile_lockcount_macro_rwl_vr_0w}
\caption{Number of lock acquisition attempts while using the reader-writer lock for a pure reader workload.}
\label{img:profile_rwl_locks_vr_0w}
\end{figure}

From Figure \ref{img:macro_vr_0w}, we see that RCU scales much better
than the reader-writer lock version. RCU is always within $1.1$ times
the vanilla version. Time taken by the reader-writer lock increases
linearly and is up to $22$ times slower than the version without
locks.

These results validate our claim that RCU is wait-free and lock-free
for readers. In the RCU version, readers do not create any
synchronization overhead. However, readers in the reader-writer lock version
access and update a shared lock variable before they access the routing
table. The state of the shared variable needs to be updated through
all cores. As the number of threads increases, the contention causes
cache line bouncing and increased usage of the processor bus
bandwidth. For the reader-writer lock, the number of lock acquisition attempts to enter
the read side critical section is shown in Figure \ref{img:profile_rwl_locks_vr_0w}. The graph shows an increasing number of lock acquisition attempts with an increase in the number of threads. We can therefore attribute the  linear increase in time to lock contention. 

\subsubsection{Read Intensive Workload With One Writer}
\begin{table}[tph]
\begin{center}
\input{macro_vr_1w.tex}
\end{center}
\caption{Performance comparison of increasing number of readers and one writer with the 167k routing table.}
\label{tbl:macro_vr_1w}
\end{table}


\begin{figure}[tph]
\centering
\includegraphics[scale = 0.7]{../images/graphs/macro_vr_1w}
\caption{Performance of increasing number of readers and one writer with the 167k routing table.}
\label{img:macro_vr_1w}
\end{figure}


\begin{figure}[tph]
\centering
\includegraphics[scale = 0.7]{../images/graphs/profile_lockcount_macro_rwl_vr_1w}
\caption{Number of lock acquisition attempts for workload with increasing number of readers and one writer while using a reader-writer lock.}
\label{img:profile_rwl_locks_vr_1w}
\end{figure}

We also conducted a benchmark which consisted of one updater and an
increasing number of readers. The results are shown in Figure
\ref{img:macro_vr_1w} and Table \ref{tbl:macro_vr_1w}.

We see from Figure \ref{img:macro_vr_1w} that RCU scales far better
than the reader-writer lock. Time taken by the reader-writer lock
increases linearly with the number of threads. The reader-writer lock
version is over nine times slower than the RCU version for the
workload consisting of 7 readers and 1 writer. Figure \ref{img:profile_rwl_locks_vr_1w} shows the number of lock acquisition attempts while using the reader-writer lock on this workload. We can attribute the linear increase in time to the increasing lock contention. RCU does not acquire any locks in this workload. We can clearly see that RCU
outperforms the reader-writer lock for a read intensive workload with some writers.

\subsubsection{Write Intensive Workloads}
We have seen that RCU outperforms the Reader-Writer lock for reader heavy workloads. For completeness, we look at some workloads which are not common for a router: writer heavy workloads. The performance of a workload with zero readers and increasing number of writers is shown in  Figure \ref{img:macro_01r_vw} and Table \ref{tbl:macro_0r_vw}. Here, we see that the Reader-Writer Lock outperforms RCU by a factor of 1.46. We attribute this to the increased memory footprint and increased overhead in management of the reclaim lists.

The performance of a workload consisting of one reader and increasing number of writers is shown in  Figure \ref{img:macro_01r_vw} and Table \ref{tbl:macro_1r_vw}. Although RCU does not scale, we see that it performs better than the Reader-Writer Lock.

It is clear that RCU is not as fabulous for write intensive workloads. These workloads are very uncommon for a typical router and we can accept this behavior.
 
\begin{table}[tph]
\begin{center}
\input{macro_0r_vw.tex}
\end{center}
\caption{Performance comparison of increasing number of writers and zero readers using the 167k routing table.}
\label{tbl:macro_0r_vw}
\end{table}

\begin{table}[tph]
\begin{center}
\input{macro_1r_vw.tex}
\end{center}
\caption{Performance comparison of increasing number of writers and one reader using the 167k routing table.}
\label{tbl:macro_1r_vw}
\end{table}


\begin{figure}[tph]
\centering
\includegraphics[scale = 0.45]{../images/graphs/macro_0r_vw}
\includegraphics[scale = 0.45]{../images/graphs/macro_1r_vw}
\caption{Write intensive workloads: Performance of increasing number of writers with no readers (left), and increasing number of writers with one reader (right). Both measurements used the 167k routing table.}
\label{img:macro_01r_vw}
\end{figure}

\pagebreak

\subsection{Micro-benchmarks}
\label{sec:microbenchmarks}
The micro-benchmarks have tests which repeatedly access a small set of addresses in a very short time span.
As with macro-benchmarks, we are interested mainly in read heavy workloads. We look at write heavy workloads for completeness.

\paragraph{Pure Reader Workload}
Here we consider a workload consisting of only readers. We look at how the reader-writer lock and RCU perform against the unmodified vanilla version.
As we can see from Table \ref{tbl:micro_vr_0w} which is represented in Figure \ref{img:micro_vr_0w}, the reader-writer lock is up to 50 times slower than a version without locks.
The reader-writer lock does not scale with an increase in the number of threads. The RCU version scales with an increase in the number of threads. The RCU version is also within 1.5 times the performance of the version without any locks.

The Reader-Writer lock spends time acquiring a lock on reads while RCU does not have any additional overhead for readers. Any additional overhead of RCU over the vanilla version is due to the overhead in checking reclaim lists and detecting quiescent states.

\begin{table}
\begin{center}
\scalebox{0.9} {
\input{micro_vr_0w.tex}
}
\end{center}
\label{tbl:micro_vr_0w}
\caption{Performance comparison over a workload with increasing number of readers and zero writers.}
\end{table}

\begin{figure}[tph]
\centering
\includegraphics[scale = 0.7]{../images/graphs/micro_vr_0w}
\caption{Performance comparison over a workload with increasing number of readers and zero writers.}
\label{img:micro_vr_0w}
\end{figure}

\paragraph{Read Intensive Workloads}
We now compare the performance of RCU over the reader-writer lock in the presence of a writer. This workload reflects typical router usage. We refer the reader to Table \ref{tbl:micro_vr_1w} and Figure \ref{img:micro_vr_1w} for the results.
RCU scales over an increase in the number of threads, however the reader-writer lock does not scale with an increase in the number of readers. This can be attributed to time spent waiting to acquire the lock before reading or writing in the Reader-Writer Lock version.

\begin{table}[tph]
\begin{center}
\input{micro_vr_1w.tex}
\end{center}
\label{tbl:micro_vr_1w}
\caption{Performance comparison of increasing number of readers with one writer.}
\end{table}

\begin{figure}[tph]
\begin{center}
\includegraphics[scale = 0.7]{../images/graphs/micro_vr_1w}
\caption{Performance comparison of increasing number of readers with one writer.}
\label{img:micro_vr_1w}
\end{center}
\end{figure}

\paragraph{Write Intensive Workloads}
We now compare the performance of RCU with the reader-writer lock for a write intensive workload. The performance of a workload with one reader and increasing number of writers is shown in Table \ref{tbl:micro_1r_vw} and Figure \ref{img:micro_01r_vw}. The performance of a workload with zero readers and increasing number of writers is shown in Table \ref{tbl:micro_0r_vw} and Figure \ref{img:micro_01r_vw}. In both cases, the reader-writer lock performs better than RCU. We attribute this to additional overhead caused due to lock contention to free stale data in quiescent states. When there are more writers, there is more stale data. Since these benchmarks target a very small set of routes, the reclamation also has contention for the same memory locations.

\begin{table}[tph]
\begin{center}
\input{micro_0r_vw.tex}
\end{center}
\label{tbl:micro_0r_vw}
\caption{Performance comparison of increasing number of writers with zero readers.}
\end{table}

\begin{figure}[tph]
\centering
\includegraphics[scale = 0.45]{../images/graphs/micro_0r_vw}
\includegraphics[scale = 0.45]{../images/graphs/micro_1r_vw}
\caption{Write Intensive workloads: Performance comparison of increasing number of writers with zero readers (left), and one reader (right).}
\label{img:micro_01r_vw}
\end{figure}

\begin{table}[tph]
\begin{center}
\input{micro_1r_vw.tex}
\end{center}
\label{tbl:micro_1r_vw}
\caption{Performance comparison of increasing number of writers with one reader.}
\end{table}

\pagebreak
\section{Conclusion}

The primary challenges in implementing RCU for Click were to
identify quiescent states in a way which could be applied to all
elements. 

We have built an RCU framework for userlevel Click invlolving a
mechanism to detect quiescent states and an API which can be used by userlevel Click elements.

We have verified through our benchmarks that this mechanism does
indeed have a very low read side overhead (within 1.1 times
non-locking performance). The RCU approach is also up to 27 times
faster than a reader-writer lock on read intensive workloads. RCU's performance is not better than that of a reader-wrtier lock for
write-intensive workloads. We find this behaviour acceptable since our
workload is IP lookups, which is read-intensive. 
\section{Future Work}

%\begin{enumerate}
%\item Implement RCU for RadixIPLookup if Radix nodes are deleted when a route is removed.
%\item Implement RCU for the Radix Tree: Currently, RCU has been only used on the vector. A coarse grained lock is acquired around updates to the Radix Tree.
%\end{enumerate}
We have experimented with the RadixIPLookup Element in Click. DirectIPLookup and RangeIPLookup are some of the other fast routing table elements in Click. It would be useful to implement a RCU based synchronization these elements as well.\\ 

Our approach uses RCU for the vector in RadixIPLookup, however a coarse grained lock is acquired for updates to the radix tree. The size of the radix tree depends on the size of the routing table, which is typically very large. Thus implementing RCU based synchronization for the radix tree would be very useful.

\appendix
\section{Source Code}
\label{sec:source}
\subsection{Getting the Source Code}
The source code is available in the \emph{rcu\_local\_epoch} branch of the repository, \\\texttt{https://github.com/mvenk/click.git}.
The microbenchmarks and macrobenchmarks can be found in the \emph{race/microbenchmarks} and \emph{race/macrobenchmarks} directories respectively.
\subsection{Running Benchmarks}
In order to run the benchmarks in multi-threaded mode, configure and install click like so:
\begin{verbatim}
$cd click_dir
$./configure --enable-user-multithread --enable-multithread=8
$ make -j 8
\end{verbatim}
Note that we configure with \emph{8}, since our experiments were run on an 8 core machine.
The Makefile in the microbenchmarks and macrobenchmarks directories contain a brief description of the tests. To run a macrobenchmark with varying number of readers and one writer, do the following:
\begin{verbatim}
$cd race/macrobenchmarks
$make rcu_vr_0w
\end{verbatim}
In order to compare against the reader-writer lock version or the non-locking version, run the same tests in the \emph{rcu\_master\_tests} branch. All tests are configured for a machine with 8 cores. In order to generate tests for N number of cores, use the \emph{gen\_benchmarks.sh} script provided in the \emph{race/macrobenchmarks} directory.
\bibliography{rcu_report}
\bibliographystyle{plain}
\end{document}

